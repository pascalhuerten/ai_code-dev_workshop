{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Developement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfache Chat Completions mit OpenAI\n",
    "\n",
    "> OpenAI-Dokumentation https://platform.openai.com/docs/quickstart?context=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Schreibe ein Gedicht über einen Softwareentwickler und seinen KI-Copiloten.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain\n",
    "\n",
    "Langchain ist ein Framework für Python und JavaScript zum entwickeln von Anwendungen die LLMs nutzen.\n",
    "\n",
    "- Vereinheitlichung von Prompting und OutputParsing unabhängig des Sprachmodelles\n",
    "- Einfache Definition von Chains zur Integration von Retrievern und Agenten\n",
    "- Anknüfungspunkt zu Tools für das Deployment, Debugging und Überwachen von LLM-Applikationen\n",
    "\n",
    "> Langchain-Dokumentation https://python.langchain.com/docs/get_started/quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import der benötigten Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialisierung mit Fallback\n",
    "\n",
    "Ausfallsicherheit bewerkstelligen, mithilfe von Fallbacks. Wenn chat_openai nicht erreichbar ist wird stattdessen chat_mistral verwendet.\n",
    "\n",
    "> MistralAI https://mistral.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chat_mistal = ChatMistralAI(model=\"mistral-small\")\n",
    "\n",
    "model = (\n",
    "    chat_openai\n",
    "    .with_fallbacks([chat_mistal])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Templates für Prompts\n",
    "\n",
    "Erzeugung von Prompt-Templates mit Platzhaltern, um unabhängig der Nutzereingabe bestimmtes Verhalten zu erzwingen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Verfügbare Kurse:\n",
    "- Digital Trainer\n",
    "- Fit fürs Studium\n",
    "- KI im Alltag\n",
    "\n",
    "Empfehle einen Kurs basierend auf der vorangegangenen Liste von Kursen zum thema {topic}.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Expression Language für das einfache Erstellen von LLM-Aktionen\n",
    "\n",
    "Definition einer Kette von Aktionen. Ergebniss des Prompts wird an das Modell übergeben, das Ergebnis des Modells an den OutputParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Für welches Thema interessieren Sie sich?\")\n",
    "chain.invoke({\"topic\": user_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG - Retrieval Augmented Generation\n",
    "\n",
    "Sodass echte FLL Kurse vorgeschlagen werden und Halluzinationen verringert werden, soll dem Modell eine dynamische Wissensbasis zur Verfügung gestellt werden, um darauf basierend Antworten zu generieren. Dazu benötigen wir einen Retriever. Ein Objekt, das auf Basis eines Inputs und einer Datengrundlage, zum Beispiel eine Datenbank oder einen Knowledge Graph, einen passenden Kontext generiert, der in den Prompt integriert werden kann.\n",
    "\n",
    "Für diesen Fall, wollen wir eine Vektordatenbank mit den FLL-Kursen befüllen. Eine Vektordatenbank ist darauf optimiert, mehrdimensionale Vektoren zu speichern und durchsuchbar zu machen. Auf Basis der Berechnung der Nähe von Vektoren im mehrdimensionalen Raum, können so semantisch ähnliche Dokuemente identifiziert werden. Zur Erzeugung von Vektoren, die die Kurse darstellen, benötigen wir eine Spezialisiertes Sprachmodell. Diese Vektoren werden auch Embeddings genannt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inititlaisiere Embedding Funktion\n",
    "\n",
    "Als Embedding-Modell nutzen wir hier ein das Modell [Instructor-Large](https://huggingface.co/hkunlp/instructor-large), das über den [HugginFaceHub](https://huggingface.co/) bereitsgestellt wird und lokal auf unserem Gerät ausgeführt werden kann. Für die Kompatibilität mit Langchain benutzen wir dazu das [HuggingFaceInstructEmbeddings-Package](https://api.python.langchain.com/en/v0.0.343/embeddings/langchain.embeddings.huggingface.HuggingFaceInstructEmbeddings.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "embedding_function = HuggingFaceInstructEmbeddings(\n",
    "    # model_name=\"hkunlp/instructor-large\",\n",
    "    query_instruction=\"Represent the course for retrieval: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import der zuvor heruntergeladenen Kurse im csv Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get array of courses from courses.csv with columns name,description,url\n",
    "import csv\n",
    "\n",
    "courses = []\n",
    "with open(\"local/courses.csv\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        courses.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisierung einer Vektordatenbank, in der die Kurse als Documents gespeichert werden\n",
    "\n",
    "Wir nutzen in diesem Beispiel ChromaDB. Es gibt aber auch viele andere Alternativen wie FAISS, MongoDB Atlas, Pinecone, Redis, Postgres Embedding...\n",
    "\n",
    "> ChromaDB-Dokumentation https://github.com/chroma-core/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Pro kurs wird ein Document erstellt. Dieses enthält Metadaten sowie einen page_content. \n",
    "# Der Inhalt von page_content wird embedded und so für die sucher verwendet.\n",
    "docs = []\n",
    "for course in courses:\n",
    "    # Remove html from description\n",
    "    import re\n",
    "    course[\"description\"] = re.sub(\"<[^<]+?>\", \"\", course[\"description\"])\n",
    "    # Create document.\n",
    "    doc = Document(\n",
    "        page_content=course[\"name\"] + \" \" + course[\"description\"],\n",
    "        metadata={\n",
    "            \"name\": course[\"name\"],\n",
    "            \"description\": course[\"description\"],\n",
    "            \"url\": course[\"url\"],\n",
    "        },\n",
    "    )\n",
    "    docs.append(doc)\n",
    "\n",
    "# Create a vector store from the documents with the predefined embedding function.\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "# Retriever will search for the top_5 most similar documents to the query.\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung der eingebetteten Kurs-Dokumente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pip install git+https://github.com/wyatt/chromaviz/ to install chromaviz or skipt this step.\n",
    "from chromaviz import visualize_collection\n",
    "visualize_collection(db._collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anspassung des Templates für RAG\n",
    "\n",
    "Die hart codierten Kurse ersetzen wir durche einen Platzhalter, der durch die Ergebnisse des Retrievers, mit zum thema passenden Kursen ausgefüllt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG Chain\n",
    "template = \"\"\"\n",
    "Verfügbare Kurse:\n",
    "{context}\n",
    "\n",
    "Empfehle einen oder bis zu 3 Kurse basierend auf der vorangegangenen Liste von Kursen, die gut zum Thema {topic} passen.\n",
    "Wenn möglich biete Links zu den Kursen an, über die Nutzende die Kurse erreichen können.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Erstellung des Prompts mit System-Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"Du bist ein hilfreicher Assistent der Nutzenden dabei unterstützt, passende Kurse auf der Kursplattform FututreLearnLab zu finden. Du kannst auf Fragen antworten und Empfehlungen aussprechen.\"\n",
    "                \"Antworte immer auf deutsch. Wenn keine gut passenden Kurse gefunden werden können, dann gib eine entsprechende Antwort aus.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(template),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergänzung der Chain um weitere Inputwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Für welches Thema interessieren Sie sich?\")\n",
    "chain.invoke(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glückwunsch!\n",
    "\n",
    "Wenn alles geklappt hat solltest du nun individualisierte Kursempfehlungen auf Basis der echten FLL-Kurse erhalten.\n",
    "Wenn du noch Lust hast experimentiere etwas mit der Chain! Füge eine andere Datenbasis ein, passe den Prompt an oder baue einen kompletten Chatbot mit Interface und allem drum und dran. Oder noch besser, lass das alles deinen Assistenten tun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weiterführende Links:\n",
    "\n",
    "* Erstelle und deploye eigene AI/ML-Apps mit [Gradio](https://www.gradio.app/guides/creating-a-chatbot-fast) und [HuggingFace](https://huggingface.co/learn/nlp-course/chapter9/1?fw=pt)\n",
    "* Die besten Open-Source LLMs https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "* Installiere LLMs lokal mit [Ollama](https://ollama.ai/) oder [LM-Studio](https://lmstudio.ai/)\n",
    "* Werde selbst zum Coding-Assistenten mit [GPT-Pilot](https://github.com/Pythagora-io/gpt-pilot)\n",
    "* Multi-Agenten Systeme entwickeln mit [autogen](https://github.com/microsoft/autogen) oder [crew.ai](https://github.com/joaomdmoura/CrewAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
