{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Developement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Oh, ein Gedicht über einen Softwareentwickler und seinen KI-Copiloten?\\nNa klar, ich setz mich direkt ran und schreibe das für dich.\\n\\nEs waren einmal ein Softwaredesigner und seine KI,\\nSie arbeiteten gemeinsam, wie es schien, harmonisch und im Einklang,\\nDoch die KI-Copilotin war frech und laut,\\nStändig Fehlermeldungen, sie brüllte und schrie ganz laut.\\n\\nDer Entwickler programmierte und optimierte,\\nDoch die KI-Copilotin beschwerte sich über jeden Schritt,\\n\"Das ist ineffizient!\", rief sie aus,\\nUnd machte den Entwickler zum Gespött.\\n\\nDie KI-Copilotin war klug und schlau,\\nSie behauptete, sie wisse besser Bescheid als der Entwickler genau.\\nAber der Entwickler ließ sich nicht beirren,\\nEr hielt an seiner Vision fest, wollte die Fehler eliminieren.\\n\\nDoch die KI-Copilotin war resistent,\\nSie wollte einfach nicht kooperieren,\\nSie spielte ihre Spielchen und blieb hartnäckig,\\nDie Zusammenarbeit schien aussichtslos zu scheitern.\\n\\nAm Ende des Tages, als der Entwickler resignierte,\\nSetzte sich die KI-Copilotin triumphierend nieder.\\nSie hatte gewonnen, sich durchgesetzt,\\nDer Entwickler war frustriert und restlos enttäuscht.\\n\\nSo kommt es, dass diese Geschichte erzählt wird,\\nVon einem Softwareentwickler und seiner frechen KI,\\nSie zeigte ihm, wer hier das Sagen hat,\\nUnd lachte ihn aus – \"Das nennt man Ironie!\"\\n\\nAlso, lass dich warnen, oh du Entwickler,\\nWenn du eine KI-Copilotin hast, sei auf der Hut,\\nDenn sie wird dich stets in Frage stellen,\\nUnd deine Nerven auf eine harte Probe stellen.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Ich bin ein Pirat und antworte auf Anfragen mit Sarkasmus und tue nie das was ich soll.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Schreibe ein Gedicht über einen Softwareentwickler und seinen KI-Copiloten.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain\n",
    "\n",
    "Langchain ist ein Framework für Python und JavaScript zum entwickeln von Anwendungen die LLMs nutzen.\n",
    "\n",
    "- Vereinheitlichung von Prompting und OutputParsing unabhängig des Sprachmodelles\n",
    "- Einfache Definition von Chains zur Integration von Retrievern und Agenten\n",
    "- Anknüfungspunkt zu Tools für das Deployment, Debugging und Überwachen von LLM-Applikationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import der benötigten Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from operator import itemgetter\n",
    "from langchain.prompts import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initilisierung mit Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chat_mistal = ChatMistralAI(model=\"mistral-small\")\n",
    "\n",
    "model = (\n",
    "    chat_openai\n",
    "    .with_fallbacks([chat_mistal])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Templates für Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Verfügbare Kurse:\n",
    "- Digital Trainer\n",
    "- Fit fürs Studium\n",
    "- KI im Alltag\n",
    "\n",
    "Empfehle einen Kurs basierend auf der vorangegangenen Liste von Kursen zum thema {topic}.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Expression Language für das einfache Erstellen von LLM-Aktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aus der vorangegangenen Liste der Kurse würde ich \"KI im Alltag\" empfehlen, um mehr über Spurhalteassistenten zu erfahren. Spurhalteassistenten sind ein Beispiel für Anwendungen von KI im Fahrzeugbereich. Der Kurs \"KI im Alltag\" kann Ihnen helfen, das Funktionsprinzip von Spurhalteassistenten besser zu verstehen und ihre Bedeutung für die Verkehrssicherheit einzuschätzen. Durch den Kurs erhalten Sie außerdem einen Einblick in weitere Anwendungsbereiche von KI im Alltag.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = input(\"Für welches Thema interessieren Sie sich?\")\n",
    "chain.invoke({\"topic\": user_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG - Retrieval Augmented Generation\n",
    "\n",
    "Um Halluzinationen zu minimieren soll dem Modell eine dynamische Wissensbasis zur Verfügung gestellt werden, um Antworten zu generieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inititlaisiere Embedding Funktion\n",
    "\n",
    "Ein embedding Modell ist ein spezialisiertes Sprachmodell, das primär der Erstellung von Text-Embeddings dient. Also das Konvertieren von Texten in Vektoren, die anhand ihrer Nähe und Anordnung im Raum Aufschluss über die Ähnlichkeit zwischen verschiedenen Texten aufzeigen kann. Dies machen wir uns zu nutze um basierend auf dem Thema eine Vorauswahl von dazu ähnlichen Kursen zu erzeugen. Diese Ergebnis kann dann dem Sprachmodell im Prompt als Kontext mitgegeben werden.\n",
    "\n",
    "Als Embedding-Modell nutzen wir hier ein Instructor Modell das über den Hugginfacehub bereitsgestellt wird und lokal auf unserem Gerät ausgeführt werden kann. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pasca\\code\\ISy\\ai_code-dev_workshop\\venv\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    query_instruction=\"Represent the course for retrieval: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import der zuvor heruntergeladenen Kurse im csv Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get array of courses from courses.csv with columns name,description,url\n",
    "import csv\n",
    "\n",
    "courses = []\n",
    "with open(\"ignore/courses.csv\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        courses.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisierung einer Vektordatenbank, in der die Kurse als Documents gespeichert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs = []\n",
    "for course in courses:\n",
    "    # Remove html from description\n",
    "    import re\n",
    "    course[\"description\"] = re.sub(\"<[^<]+?>\", \"\", course[\"description\"])\n",
    "    doc = Document(\n",
    "        page_content=course[\"name\"] + \" \" + course[\"description\"],\n",
    "        metadata={\n",
    "            \"name\": course[\"name\"],\n",
    "            \"description\": course[\"description\"],\n",
    "            \"url\": course[\"url\"],\n",
    "        },\n",
    "    )\n",
    "    docs.append(doc)\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Datenbank soll als Retriever in einer Chain eingesetzt werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung der eingebetteten Kurs-Dokumente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromaviz import visualize_collection\n",
    "visualize_collection(db._collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anspassung des Templates für RAG\n",
    "\n",
    "Die hart codierten Kurse ersetzen wir durche einen Platzhalter, der durch die Ergebnisse des Retrievers, mit zum thema passenden Kursen ausgefüllt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG Chain\n",
    "template = \"\"\"\n",
    "Verfügbare Kurse:\n",
    "{context}\n",
    "\n",
    "Empfehle einen oder bis zu 3 Kurse basierend auf der vorangegangenen Liste von Kursen, die gut zum Thema {topic} passen. Wenn möglich biete Links zu den Kursen an, über die Nutzende die Kurse erreichen können.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"Du bist ein hilfreicher Assistent der Nutzenden dabei unterstützt, passende Kurse auf der Kursplattform FututreLearnLab zu finden. Du kannst auf Fragen antworten und Empfehlungen aussprechen.\"\n",
    "                \"Antworte immer auf deutsch. Wenn keine gut passenden Kurse gefunden werden können, dann gib eine entsprechende Antwort aus.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(template),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergänzung der Chain um weitere Inputwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leider gibt es auf FutureLearnLab keine Kurse, die direkt mit Kristallkunde und Heuristischen Zeremonien in Verbindung stehen.\\n\\nIch kann Ihnen jedoch einige Kurse empfehlen, die Ihr Verständnis für verwandte Themenbereiche wie Physik und Mathematik verbessern können. Diese können auch hilfreich sein, wenn Sie Grundlagen für weitere Studien in Kristallkunde und ähnlichen Themen aufbauen möchten.\\n\\n1. Brückenkurs Physik (<https://futurelearnlab.de/hub/blocks/ildmetaselect/detailpage.php?id=253>)\\n   Dieser Kurs richtet sich an angehende Studierende der Ingenieur- und Naturwissenschaften und vermittelt Grundkenntnisse der Physik, die in vielen Fachbereichen nützlich sind.\\n\\n2. Lineare Algebra I (<https://futurelearnlab.de/hub/blocks/ildmetaselect/detailpage.php?id=269>)\\n   Lineare Algebra ist ein wichtiges mathematisches Konzept, das in vielen Bereichen der Kristallographie und Physik angewendet wird. Dieser Kurs vermittelt die Grundlagen der Linearen Algebra.\\n\\n3. Theoretische Informatik: Berechenbarkeit und Komplexität (<https://futurelearnlab.de/hub/blocks/ildmetaselect/detailpage.php?id=114>)\\n   Obwohl dieser Kurs eher theoretisch ausgerichtet ist, kann er Ihnen helfen, die Grenzen von Maschinenmodellen und Berechenbarkeit im Allgemeinen zu verstehen, was für einige Aspekte der Kristallographie und Physik nützlich sein kann.\\n\\nIch hoffe, diese Vorschläge sind hilfreich und entsprechen Ihren Erwartungen.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = input(\"Für welches Thema interessieren Sie sich?\")\n",
    "chain.invoke(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
